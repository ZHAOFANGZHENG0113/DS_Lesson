{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11 Tasks\n",
    "\n",
    "A common text classification task involves automatically determining the language in which a document is written, based on previously-labelled example documents.\n",
    "\n",
    "In this notebook, we will look at automatically classifying the text from tweets as either English or non-English. The dataset we will use is a subset of the [UMass Global English on Twitter Dataset](https://www.kaggle.com/rtatman/the-umass-global-english-on-twitter-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Preprocessing\n",
    "\n",
    "Read the Twitter dataset from the CSV file 'tweet-language.tsv' into a Pandas DataFrame, where the row index is given by 'Tweet Id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 6759 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tweet ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>285903159434563584</th>\n",
       "      <td>volkan konak adami tribe sokar yemin ederim :d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285965965118824448</th>\n",
       "      <td>i felt my first flash of violence at some fool...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286057979831275520</th>\n",
       "      <td>ladies drink and get in free till 10:30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286216100784521216</th>\n",
       "      <td>watching #miranda on bbc1!!! u r hilarious</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286525170670243840</th>\n",
       "      <td>all over twitter because you and your friends ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286916662836490241</th>\n",
       "      <td>~ i'm falling apart,with a broken heart,barely...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286927073078018048</th>\n",
       "      <td>oh my god, we go way back #lovethis #rahrahrah</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286927433498759168</th>\n",
       "      <td>the way you treat me. the way you accept me, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286941320851890177</th>\n",
       "      <td>i just wanna get pulled on the sled by the fou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286949739071672321</th>\n",
       "      <td>oh there's that fake door slam noise , ugh #cbb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                Tweet  English\n",
       "Tweet ID                                                                      \n",
       "285903159434563584     volkan konak adami tribe sokar yemin ederim :d        0\n",
       "285965965118824448  i felt my first flash of violence at some fool...        1\n",
       "286057979831275520            ladies drink and get in free till 10:30        1\n",
       "286216100784521216         watching #miranda on bbc1!!! u r hilarious        1\n",
       "286525170670243840  all over twitter because you and your friends ...        1\n",
       "286916662836490241  ~ i'm falling apart,with a broken heart,barely...        1\n",
       "286927073078018048     oh my god, we go way back #lovethis #rahrahrah        1\n",
       "286927433498759168  the way you treat me. the way you accept me, a...        1\n",
       "286941320851890177  i just wanna get pulled on the sled by the fou...        1\n",
       "286949739071672321    oh there's that fake door slam noise , ugh #cbb        1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweet-language.tsv\", sep=\"\\t\").set_index(\"Tweet ID\")\n",
    "print(\"Read %d documents\" % len(df))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target label for classification here is going to be the column 'English' -- a value of 1 indicates that a tweet is in English, while a value of 0 indicates it is written in another language.\n",
    "\n",
    "From this column, check the number of tweets in the dataset for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_categories = [\"Non-English\", \"English\"]\n",
    "documents = df[\"Tweet\"]\n",
    "target = df[\"English\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the DataFrame and functionality from scikit-learn, create a vector representations of the documents. For real applications we would want to use a custom tokenizer to handle the specifics of tweets (e.g. mentions, hashtags etc). However, for this example we can just use the standard scikit-learn tokenizer and a simple *CountVectorizer*. \n",
    "\n",
    "Note that we should not use any \"stop words\" here. For language detection, common stop words might actually prove to be useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6759, 892)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df = 10, stop_words=None)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 892 distinct terms\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['central' 'centre' 'centro' 'change' 'che' 'check' 'chicago' 'chile'\n",
      " 'christmas' 'city' 'cl' 'clerical' 'click' 'cloudy' 'club' 'co' 'coffee'\n",
      " 'coisa' 'com' 'come']\n"
     ]
    }
   ],
   "source": [
    "print(terms[150:170])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Classification and Train/Test Evaluation\n",
    "\n",
    "Train a kNN classification model with 3 neighbours, and evaluate the accuracy of this model using a single train/test split, so that we have 70% of the tweets in the training set and 30% in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(X, target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the classification and evaluation process again using a different train/test split. Did the classifier achieve the same accuracy score as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8511\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(data_train, target_train)\n",
    "predicted = model.predict(data_test)\n",
    "print(\"Accuracy = %.4f\" % accuracy_score(target_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Classification and Cross-Validation\n",
    "\n",
    "If we re-run the evaluation above several times, we will get different performance scores depending on the randomly-generated training/test split that we are using. A more robust strategy involves using *k-fold cross-validation* to evaluate a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the kNN classifier from above, but this time using 5-fold cross validation. The model in each fold should be evaluated using accuracy. Calculate the overall average accuracy across all 5 folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline1 = Pipeline([\n",
    "    ('vec', CountVectorizer(min_df = 10, stop_words=None)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', KNeighborsClassifier(n_neighbors=3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7029\n"
     ]
    }
   ],
   "source": [
    "acc_scores = cross_val_score(pipeline1, documents, target, cv=5, scoring=\"accuracy\")\n",
    "s_acc = pd.Series(acc_scores)\n",
    "print(\"Mean accuracy: %.4f\" % s_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline([\n",
    "    ('vec', CountVectorizer(stop_words=\"english\")),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.9072\n"
     ]
    }
   ],
   "source": [
    "acc_scores = cross_val_score(pipeline2, documents, target, cv=5, scoring=\"accuracy\")\n",
    "s_acc = pd.Series(acc_scores)\n",
    "print(\"Mean accuracy: %.4f\" % s_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
